# -*- coding: utf-8 -*-
"""AlphaPrecision_BetaRecall_EvalMetric.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cT2I5-vTYdqk7OQrsteeA1ikfPl2DQaf

# Based on the Alaa 2022 Code
"""

"""# Function Imports"""

from metrics.prdc import compute_prdc
import numpy as np
import matplotlib.pyplot as plt
import torch
import tensorflow as tf
import pickle as pkl
import numpy as np
import io
import pandas as pd


from representations.OneClass import * 
from metrics.evaluation import *

nearest_k = 5
params  = dict({"rep_dim": None, 
                "num_layers": 2, 
                "num_hidden": 200, 
                "activation": "ReLU",
                "dropout_prob": 0.5, 
                "dropout_active": False,
                "train_prop" : 1,
                "epochs" : 100,
                "warm_up_epochs" : 10,
                "lr" : 1e-3,
                "weight_decay" : 1e-2,
                "LossFn": "SoftBoundary"})   

hyperparams = dict({"Radius": 1, "nu": 1e-2})



def plot_all(x, res, x_axis):
    print(x_axis)
    if type(res) == type([]):
        plot_legend = False
        res = {'0':res}
    else:
        plot_legend = True
    exp_keys = list(res.keys())
    print(res)
    metric_keys = res[exp_keys[0]][0].keys() 
    for m_key in metric_keys:
        for e_key in exp_keys:
          y = [res[e_key][i][m_key] for i in range(len(x))]
          plt.plot(x, y, label=e_key)
        
        plt.ylabel(m_key)
        plt.ylim(bottom=0)
        plt.xlabel(x_axis) 
        if plot_legend:
            plt.legend()
        plt.show()


def compute_metrics(X,Y, nearest_k = 5, model = None):
    # print(type(X))
    # print(type(Y))
    results = compute_prdc(X,Y, nearest_k)
    if model is None:
        #these are fairly arbitrarily chosen
        params["input_dim"] = X.shape[1]
        params["rep_dim"] = X.shape[1]        
        hyperparams["center"] = torch.ones(X.shape[1])
        # print(type(model))
        model = OneClassLayer(params=params, hyperparams=hyperparams)
        # print(type(model))
        model.fit(X,verbosity=False)

    X_out = model(torch.tensor(X).float()).float().detach().numpy()
    Y_out = model(torch.tensor(Y).float()).float().detach().numpy()

    # sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor)
    # attempted non true version
    # trying to put it to a torch
    # error on float not being a double so changed float to a double -- not possible
    # maybe an issue with types? trying to print out the types now
    # Current error is occuring with the model function. Let's see where that's coming from

    #X_ten = torch.tensor(X)
    #Y_ten = torch.tensor(Y)

    #print(type(X_ten))
    #print(type(Y_ten))

    #print(type( X_ten.clone().detach() ))

    #X_test = model(X_ten.clone().detach())

    #X_out = model(X_ten.clone().detach()).float().detach().numpy()
    #Y_out = model(Y_ten.clone().detach()).float().detach().numpy()
    
    # print(type(X_out))
    # print(type(Y_out))
    # print(type(model.c))

    #alphas, alpha_precision_curve, beta_coverage_curve, Delta_precision_alpha, Delta_coverage_beta, (thresholds, authen) = compute_alpha_precision(X_out, Y_out, model.c)
    alphas, alpha_precision_curve, beta_coverage_curve, Delta_precision_alpha, Delta_coverage_beta, a = compute_alpha_precision(X_out, Y_out, model.c)
    # print(a)
    results['Dpa'] = Delta_precision_alpha
    results['Dcb'] = Delta_coverage_beta
    # results['mean_aut'] = np.mean(authen)
    results['mean_aut'] = np.mean(a)        # I don't know if this would change the program intent
    return results, model

"""# Define Orig Data and Gen Data

Take pickle data from gen model and use metrics to compare generated data to stair distribution.
"""

def get_100_pickle():
  # pulls 100sample.pk file in and puts into np array
  # shape: (100000, 6) <class 'numpy.ndarray'>
  
  infile = open('100sample.pk','rb')
  hund_pickle_samples = pkl.load(infile)

  print(hund_pickle_samples)
  print(hund_pickle_samples.shape)
  print(type(hund_pickle_samples))

  return hund_pickle_samples

def get_1_pickle():
    # pulls 100sample.pk file in and puts into np array
    # shape: (100000, 6) <class 'numpy.ndarray'>
    print("getting 1sample.pk")

    infile = open('1sample.pk','rb')
    one_pickle_sample = pkl.load(infile)

    print(one_pickle_sample)
    print(one_pickle_sample.shape)
    print(type(one_pickle_sample))

    return one_pickle_sample

def make_arr_small(X):
    return X[:50,:]

def get_stair():
  # placeholder, returns dummy array
  # shape: (100000, 6) <class 'numpy.ndarray'>

  test = np.ones((100000, 6))
  
  print(test)
  print(test.shape)
  print(type(test))

  return test

get_100_pickle()
get_stair()

# 100 sample results is a histogram, not the "ground truth"
# hund_results_file = open('100sample_result.pk','rb')
# hund_results = pkl.load(hund_results_file)
# print(hund_results)
# print(type(hund_results))

# Need something to compare it to here
# Some of the samples are "likely"
# Some of the samples are "rare"
# Maybe sort them into "likely and rare" categories and then compare metrics to the stair case?


"""# Calling Compute Metrics"""

# This is a method from the "toy metric evaluation code"
# This code is included here as example code
def translation_test(d=64, n=1000, step_size=0.1):
    X = np.random.randn(n,d) # returns samples from normal dist, 
    Y_0 = np.random.randn(n,d)
    print(X.shape)
    print(Y_0.shape)

    X_outlier = X.copy()
    X_outlier[-1] = np.ones(d)

    res = []
    res_outr = []
    res_outf = []

    # translation
    mus = np.arange(0,0.2+step_size,step_size)
    model = None
    for i, mu in enumerate(mus):
        Y = Y_0 + mu
        res_, model = compute_metrics(X,Y, model=model)
        res.append(res_)

    plot_all(mus, res, r'$\mu$')

def call_compute_metrics(n,d):
    X = np.random.randn(n,d) # returns samples from normal dist, 
    Y = np.random.randn(n,d)
    print(X)
    
    print(X.shape)
    # print(Y.shape)
    
    model = None
    res_, model = compute_metrics(X,Y, model=model)
    
    return res_

# result = call_compute_metrics(n=10000, d=64)
# print(result)

def compare(X,Y):
    model = None
    res_, model = compute_metrics(X,Y, model=model)
    
    return res_

result = compare(make_arr_small(get_1_pickle()),make_arr_small(get_stair()))
print(result)